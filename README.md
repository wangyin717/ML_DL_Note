# 机器学习
## 1 梯度

### 1.1 代价函数

![image-20230727145020689](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727145020689.png)

### 1.2 梯度下降

梯度下降是一个用来求函数最小值的算法，使用梯度下降算法来求出代价函数 𝐽(𝜃0, 𝜃1) 的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合(𝜃0, 𝜃1, . . . . . . , 𝜃n )，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。

![image-20230725114347177](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230725114347177.png)

其中𝑎是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。

### 1.3 特征缩放

特征尺度差距太大会影响梯度下降算法的效率，因此需要将所有特征的尺度尽量缩放到-1到1之间。

![image-20230727142355849](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727142355849.png)

### 1.4 正规方程

![image-20230727143301540](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727143301540.png)

总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数𝜃的替代方法。 具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。对于图像来说，特征数量肯定大于一万：224×224 = 50176



## 2 逻辑回归

### 2.1 逻辑回归的代价函数

逻辑回归的假设函数ℎ就是sigmoid函数，带入到代价函数会得到一个非凸函数，意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。![image-20230727145313296](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727145313296.png)

因此需要重新定义代价函数：

![image-20230727145341491](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727145341491.png)

真实值等于1时，预测值（x轴）越接近1，loss越小。

真实值等于0时，预测值越接近0，loss越小。

### 2.2 多分类问题

https://zhuanlan.zhihu.com/p/405545756

![image-20230727152000979](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727152000979.png)

![image-20230727152012498](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727152012498.png)

### 2.3 过拟合

![image-20230727152557191](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727152557191.png)

### 2.4 正则化

正则化的目的是防止模型过拟合，具体做法是给高次项（如上图x<sup>3</sup>和x<sup>4</sup>）的参数设置惩罚:

![image-20230727154048088](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727154048088.png)

如果特征很多，不知道惩罚哪一个，就对所有特征进行惩罚：

![image-20230727154220559](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727154220559.png)

### 2.5 EMD Loss

EMD距离度量两个分布之间的距离

## 3 神经网络

### 3.1 特征

从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中， 我们使用数据中的原始特征𝑥1, 𝑥2, . . . , 𝑥𝑛，虽然可以使用一些二项式项来组合 这些特征，但是我们仍然受到这些原始特征的限制。

在神经网络中，原始特征只是输入层。

<img src="https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731110014689.png" alt="image-20230731110014689" style="zoom: 67%;" />



在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特 征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得 出的一系列用于预测输出变量的新特征。

### 3.2 反向传播

![image-20230731111216554](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731111216554.png)

![image-20230731112549282](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731112549282.png)

> **BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过逐层处理并传向输出层。如果在输出层得不到期望的输出值，则通过构造输出值与真实值的损失函数作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯度，作为修改权值的依据，网络的学习在权值修改过程中完成。输出值与真实值的误差达到所期望值时，网络学习结束。**

https://zhuanlan.zhihu.com/p/261710847

### 3.3 诊断偏差和方差

当运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况： 要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。

![image-20230731141935765](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731141935765.png)

对于训练集，当 𝑑 较小时，模型拟合程度更低，误差较大；随着 𝑑 的增长，拟合程度提高，误差减小。 

对于交叉验证集，当 𝑑 较小时，模型拟合程度低，误差较大；但是随着 𝑑 的增长， 误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。

![image-20230731142147681](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731142147681.png)

![image-20230731142135011](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731142135011.png)

最佳结果应该是在验证集误差最低的时候。

Q：为什么在一些美学评价网络中没有用正则化方法来防止过拟合？

A：Dropout也是一种正则化方法。

- 高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。
- 高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。

### 3.4 学习曲线

学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（𝑚）的函数绘制的图表。

当训练集得分和验证集得分之间存在明显差距时，例如当验证误差（损失）在某个点开始增加而训练误差（损失）仍在减少时，我们可以判定模型发生了过拟合

![image-20230731163707883](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731163707883.png)

### 3.5 决定下一步做什么

- 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小。
- 使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算 代价比较大，但是可以通过正则化手段来调整而更加适应数据。

**通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。**



## 4 机器学习系统的设计

### 4.1 支持向量机SVM

![image-20230731174611516](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731174611516.png)

当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。 然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间 距来分离样本。



## 5.无监督学习

### 5.1 聚类

k-Means算法：

1. 首先选择𝐾个随机的点，称为聚类中心（cluster centroids）；
2. 对于数据集中的每一个数据，按照距离𝐾个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。
3. 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
4. 重复步骤 2-4 直至中心点不再变化。

### 5.2 降维

PCA主成分分析法

![image-20230731191443215](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731191443215.png)

