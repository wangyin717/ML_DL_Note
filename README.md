# 机器学习
## 1 梯度

### 1.1 代价函数

![image-20230727145020689](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727145020689.png)

### 1.2 梯度下降

梯度下降是一个用来求函数最小值的算法，使用梯度下降算法来求出代价函数 𝐽(𝜃0, 𝜃1) 的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合(𝜃0, 𝜃1, . . . . . . , 𝜃n )，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。

![image-20230725114347177](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230725114347177.png)

其中𝑎是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。

### 1.3 特征缩放

特征尺度差距太大会影响梯度下降算法的效率，因此需要将所有特征的尺度尽量缩放到-1到1之间。

![image-20230727142355849](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727142355849.png)

### 1.4 正规方程

![image-20230727143301540](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727143301540.png)

总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数𝜃的替代方法。 具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。对于图像来说，特征数量肯定大于一万：224×224 = 50176



## 2 逻辑回归

### 2.1 逻辑回归的代价函数

逻辑回归的假设函数ℎ就是sigmoid函数，带入到代价函数会得到一个非凸函数，意味着我们的代价函数有许多局部最小值，这将影响梯度下降算法寻找全局最小值。![image-20230727145313296](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727145313296.png)

因此需要重新定义代价函数：

![image-20230727145341491](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727145341491.png)

真实值等于1时，预测值（x轴）越接近1，loss越小。

真实值等于0时，预测值越接近0，loss越小。

### 2.2 多分类问题

https://zhuanlan.zhihu.com/p/405545756

![image-20230727152000979](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727152000979.png)

![image-20230727152012498](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727152012498.png)

### 2.3 过拟合

![image-20230727152557191](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727152557191.png)

### 2.4 正则化

正则化的目的是防止模型过拟合，具体做法是给高次项（如上图x<sup>3</sup>和x<sup>4</sup>）的参数设置惩罚:

![image-20230727154048088](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727154048088.png)

如果特征很多，不知道惩罚哪一个，就对所有特征进行惩罚：

![image-20230727154220559](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230727154220559.png)

### 2.5 EMD Loss

EMD距离度量两个分布之间的距离

## 3 神经网络

### 3.1 特征

从本质上讲，神经网络能够通过学习得出其自身的一系列特征。在普通的逻辑回归中， 我们使用数据中的原始特征𝑥1, 𝑥2, . . . , 𝑥𝑛，虽然可以使用一些二项式项来组合 这些特征，但是我们仍然受到这些原始特征的限制。

在神经网络中，原始特征只是输入层。

<img src="https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731110014689.png" alt="image-20230731110014689" style="zoom: 67%;" />



在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特 征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得 出的一系列用于预测输出变量的新特征。

### 3.2 反向传播

![image-20230731111216554](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731111216554.png)

![image-20230731112549282](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731112549282.png)

> **BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过逐层处理并传向输出层。如果在输出层得不到期望的输出值，则通过构造输出值与真实值的损失函数作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯度，作为修改权值的依据，网络的学习在权值修改过程中完成。输出值与真实值的误差达到所期望值时，网络学习结束。**

https://zhuanlan.zhihu.com/p/261710847

### 3.3 诊断偏差和方差

当运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况： 要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。

![image-20230731141935765](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731141935765.png)

对于训练集，当 𝑑 较小时，模型拟合程度更低，误差较大；随着 𝑑 的增长，拟合程度提高，误差减小。 

对于交叉验证集，当 𝑑 较小时，模型拟合程度低，误差较大；但是随着 𝑑 的增长， 误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。

![image-20230731142147681](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731142147681.png)

![image-20230731142135011](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731142135011.png)

最佳结果应该是在验证集误差最低的时候。

Q：为什么在一些美学评价网络中没有用正则化方法来防止过拟合？

A：Dropout也是一种正则化方法。

- 高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。
- 高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。

### 3.4 学习曲线

学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（𝑚）的函数绘制的图表。

当训练集得分和验证集得分之间存在明显差距时，例如当验证误差（损失）在某个点开始增加而训练误差（损失）仍在减少时，我们可以判定模型发生了过拟合

![image-20230731163707883](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731163707883.png)

### 3.5 决定下一步做什么

- 使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小。
- 使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算 代价比较大，但是可以通过正则化手段来调整而更加适应数据。

**通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。**



## 4 机器学习系统的设计

### 4.1 支持向量机SVM

![image-20230731174611516](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731174611516.png)

当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。 然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间 距来分离样本。



## 5.无监督学习

### 5.1 聚类

k-Means算法：

1. 首先选择𝐾个随机的点，称为聚类中心（cluster centroids）；
2. 对于数据集中的每一个数据，按照距离𝐾个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。
3. 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
4. 重复步骤 2-4 直至中心点不再变化。

### 5.2 降维

PCA主成分分析法

![image-20230731191443215](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230731191443215.png)


## 1 深度学习基础

### 1.1 局部最小值与鞍点

小批量梯度下降和动量发都可以对抗局部最小值和鞍点。

引入动量后，每次在移动参数的时候，不是只往梯度的反方向来移动参数，而是根据梯度 的反方向加上前一步移动的方向决定移动方向。

动量不是只看梯度， 还看前一步的方向。即使梯度方向往左走，但如果前一步的影响力比梯度要大，球还是有可能 继续往右走，甚至翻过一个小丘，也许可以走到更好的局部最小值，这就是动量有可能带来的 好处 。

<img src="https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230802174902380.png" alt="image-20230802174902380" style="zoom: 80%;" />

### 1.2 Adam优化器

Adam是一种基于梯度下降算法的自适应学习率优化器。

> 1. 自适应调整学习率：Adam 优化器可以根据历史梯度信息来自适应地调节学习率，使得在训练初期使用较大的学习率，能够快速收敛，在训练后期使用较小的学习率，能够更加准确地找到损失函数的最小值。
> 2. 调整动量：Adam 优化器能够调整动量参数，以平衡上一次梯度和当前梯度对参数更新的影响，从而避免过早陷入局部极小值。
> 3. 归一化处理：Adam 优化器对参数的更新进行了归一化处理，使得每个参数的更新都有一个相似的量级，从而提高训练效果。
> 4. 防止过拟合：Adam 优化器结合了L2正则化的思想，在更新时对参数进行正则化，从而防止神经网络过度拟合训练数据。

总体来说，Adam 优化器能够快速、准确地最小化损失函数，提高深度神经网络的训练效果和泛化能力。
### 1.3 批量归一化（BN）

使特征里不同的维度具有同样的数值范围。

对于每个维度 i，计算其平均值 mi 和标准差 σi。

![image-20230803105611160](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230803105611160.png)

特征归一化，放在激活函数之前，之后都是可以的，在实现上，没有太大的差别。

测试的时候batch_size=1，因此归一化的时候直接用训练时计算的均值和方差

### 1.4 卷积

![image-20230803113809917](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230803113809917.png)

但是这样参数量太大，因此有了感受野。卷积神经网络会设定一个区域叫做感受野（receptive field），每个神经元都只关心自己的感受野里面发生的事情

另一个问题：同样的模式可能会出现在图像的不同区域

![image-20230803114202887](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230803114202887.png)

解决方法：共享参数，让不同感受野的神经元共享参数。

![image-20230803114412447](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230803114412447.png)

感受野加上参数共享就是卷积层（convolutional layer），用到卷积层的网络就叫卷积神经网络。

全连接层可以做各式各样的事情，它可以有各式各样的变化，但它可能没有办法在任何特定的任务上做好。而卷积层是专门为图像设计的，感受野、参数共享都是为图像设计的。

<img src="https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230803114942106.png" alt="image-20230803114942106" style="zoom:67%;" />

### 1.5 RNN

有记忆的神经网络称为循环神经网络（Recurrent Neural Network，RNN）。在 RNN 里面，每一次隐藏层的神经元产生输出的时候，该输出会被存到记忆元（memory cell）。

下一次有输入时，这些神经元不仅会考虑输入 x1, x2，还会考虑存到记忆元里的值 。

有了记忆元以后，输入同一个单词，希望输出不同的问题就有可能被解决。如图所示，同样是输入“Shanghai”这个单词，但是因为红色“Shanghai”前接了“leave”，绿色 “Shanghai”前接了“arrive”，“leave”和“arrive”的向量不一样，隐藏层的输出会不同，所以存在记忆元里面的值会不同。现在虽然 x2 的值是一样的，因为存在记忆元里面的值不同，所以隐藏层的输出会不同，所以最后的输出也就会不一样。

<img src="https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230803145318258.png" alt="image-20230803145318258" style="zoom: 80%;" />

### 1.6 LSTM

LSTM 是比较复杂的。LSTM 有三个门（gate），当外界某个神经元的输出想要被写到记忆元里面的时候，必须通过一个输入门（input gate），输入门要被打开的时候，才能把值写到记忆元里面。如果把这个关起来的话，就没有办法把值写进去。至于输入门的开关是神经网络自己学的，其可以自己学什么时候要把输入门打开，什么时候要把输入门关起来。

## 2 自注意力机制

![image-20230804161959594](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230804161959594.png)

![image-20230804162019390](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230804162019390.png)

![image-20230804162051836](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230804162051836.png)

![image-20230804162110782](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230804162110782.png)

![image-20230804170724837](https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230804170724837.png)

### 2.1 自注意力与卷积神经网络对比

> 卷积神经网络就是自注意力的特例。
>
> 在卷积神经网络里面，我们要划定感受野，也就是卷积核的大小，而感卷积的大小是人决定的。
>
> 而用自注意力去找出相关的像素，就好像是感受野是自动被学出来的，网络自己决定感受野的形状。

<img src="https://github.com/wangyin717/ML_DL_Note/blob/main/img/image-20230804172837928.png" alt="image-20230804172837928" style="zoom: 80%;" />

### 2.2 自注意力与图神经网络的关系

之前在做自注意力的时候，所谓的关联性是网络自己找出来的。但是现在既然有了图的信息，关联性就不需要机器自动找出来，图上面的边已经暗示了节点跟节点之间的关联性。所以当把自注意力用在图上面的时候，我们可以在计算注意力矩阵的时候，只计算有边相连的节点就好。所以如果领域知识告诉我们这两个向量之间没有关联，就没有必要再用机器去学习这件事情。当把自注意力按照这种限制用在图上面的时候，其实就是一种图神经网络（Graph Neural Network，GNN）。

# 面试题
### SGD和Adam优化器
随机梯度下降SGD每次计算一个batch内的数据的梯度并进行更新，SGD的学习率不变，因此选择合适的lr比较困难，太小会导致收敛速度慢，太大会使loss在最小值附近震荡。
Momentum 即动量，该改进是在SGD的基础上考虑了动量的因素，通过动量值来克服SGD易于震荡的缺点。第n次梯度更新的梯度包含了前n-1次的梯度。
Adam优化器通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率

### 为什么引入非线性激活函数
深度学习的前提是神经网络的隐层加上了非线性激活函数，提升了模型的非线性表达能力，使得神经网络可以逼近任意复杂的函数。假如有一个100层的全连接神经网络，其隐层的激活函数都是线性的，则从输入层到输出层实际上可以用一层全连接来等价替换，这样就无法实现真正的深度学习。

### 为什么ReLu要好过于tanh和sigmoid
采用Sigmoid等函数，算激活函数时（指数运算），计算量大，反向传播求误差梯度时，求导涉及求导和指数运算，计算量相对大，而采用ReLU激活函数时，整个过程的计算量节省很多。
ReLU会使一部分神经元的输出为0，这样就造成了网络的洗属性，并且减少了参数的相互依存关系，缓解了过拟合问题的发生

### Batch Normalization为什么效果好
数据经过归一化和标准化后可以加快梯度下降的求解速度，这就是Batch Normalization等技术非常流行的原因。
